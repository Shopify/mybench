
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Detailed design documentation &#8212; mybench  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Tutorial: Writing a custom benchmark" href="writing-a-benchmark.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="detailed-design-documentation">
<span id="detailed-design-doc"></span><h1>Detailed design documentation<a class="headerlink" href="#detailed-design-documentation" title="Permalink to this heading">¶</a></h1>
<section id="mybench-a-high-performance-framework-for-rapid-prototyping-of-benchmarks">
<h2>mybench: a high-performance framework for rapid prototyping of benchmarks<a class="headerlink" href="#mybench-a-high-performance-framework-for-rapid-prototyping-of-benchmarks" title="Permalink to this heading">¶</a></h2>
<p>November 2022</p>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h3>
<p>Database performance analysis is a difficult and time-consuming aspect of the
maintenance and evolution of today’s cloud-based applications. One common
method of conducting performance analysis is to benchmark the database with a
simulated load that matches what is observed in production. Database benchmarks
are typically conducted against database servers that are created in a
controlled environment with similar specification and topology as the
production databases. A simulated data set is then loaded into the database
servers. Finally, a simulated workload is sent to the database servers. The
resulting throughput and latency metrics are recorded and subsequently analyzed
to identify limitations of the system and opportunities for optimization.</p>
<p>To properly perform these benchmarks, implementations of the simulated data
sets and workloads must realistically represent the production system. Since
most modern web applications are somewhat uniquely implemented, custom
simulated data set and workload must be modeled to accurately represent
each application. Since many modern web applications consist of a number of
“microservices” each with its own codebase, databases, and query patterns, many
simulated data sets and workloads must be modeled and benchmarked to
properly gain insights into the performance of the application as a whole. The
benchmark tool must therefore provide an easy-to-use and ergonomic API to
model these data sets and workloads. This is termed as the <em>pre-processing</em>
stage of the benchmark <a class="footnote-reference brackets" href="#fpreprocessing" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<p>Following the pre-processing stage, the simulated workload must be executed by
the benchmarking tool in the processing stage. One technique to discover the
limits of the system is to drive the workload with increasing rate until
throughput saturates and latency spikes. This requires the benchmark tool to
precisely and accurately control the rate at which the workload is sent to the
database. As the rate increases, the amount of work the benchmark tool has to
do also increases, which can negatively and artificially affect the throughput
and latency. To avoid this, the benchmark tool itself must be very efficient,
and ideally detects that its own performance is compromised by the high desired
rate. To shorten the feedback loop in case something is wrong with the
benchmarking setup, it is also important for the tool to provide a live
monitoring of throughput and latency values such that engineers can detect
problems while the benchmark is in progress and abort the run if necessary.</p>
<p>Once the benchmark completes, a large amount of data would be generated, and
they must be analyzed in the post-processing stage. Throughput and latency
values collected through the benchmark should be visualized in a standardized
manner such that results from different benchmarks can be more easily compared. The
benchmark tool should thus provide a standard set of visualization and analysis
tool to speed up the interpretation of the benchmark results.</p>
<p>While a number of database benchmark tools such as sysbench <a class="reference internal" href="#sysbench01" id="id2"><span>[SYSBENCH01]</span></a>,
benchbase <a class="reference internal" href="#benchbase01" id="id3"><span>[BENCHBASE01]</span></a>, and linkbench <a class="reference internal" href="#linkbench01" id="id4"><span>[LINKBENCH01]</span></a>, none of these tools
fulfill all the requirements outlined above. The work presented here is our
attempt to solve all of these problems in a single software package we named
“mybench”.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fpreprocessing" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>The authors borrow the terminology of “pre-processing”,
“processing”, and “post-processing” from solid and fluid dynamic
simulations. In those fields, pre-processing is the stage where engineers
create the model that represents the physical system being simulated,
processing is the stage when the calculations are made, and post-processing
is the stage where analysis are performed on the resulting data. In that
industry, software packages typically include software for pre-processing
and post-processing to help engineers obtain results faster.</p>
</aside>
</aside>
<div role="list" class="citation-list">
<div class="citation" id="sysbench01" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">SYSBENCH01</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/akopytov/sysbench">https://github.com/akopytov/sysbench</a></p>
</div>
<div class="citation" id="benchbase01" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">BENCHBASE01</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/cmu-db/benchbase">https://github.com/cmu-db/benchbase</a></p>
</div>
<div class="citation" id="linkbench01" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">LINKBENCH01</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/facebookarchive/linkbench">https://github.com/facebookarchive/linkbench</a></p>
</div>
</div>
</section>
<section id="requirements">
<h3>Requirements<a class="headerlink" href="#requirements" title="Permalink to this heading">¶</a></h3>
<p>The following requirements were developed during the design process of mybench:</p>
<ul class="simple">
<li><p>The software should get out of the user’s way where possible.</p>
<ul>
<li><p>Benchmark prototyping is convenient and ergonomic to encourage users to
develop custom benchmarks.</p></li>
<li><p>Running a benchmark is convenient and ergonomic.</p></li>
</ul>
</li>
<li><p>Executing mybench-internal code should not impact the throughput and latency
of the workloads.</p></li>
<li><p>Multiple workloads can be mixed to better emulate production traffic where
multiple groups of query patterns exist.</p></li>
<li><p>Accurate and precise control of the query (a.k.a. event) rates for each
workload.</p></li>
<li><p>Live monitoring of throughput and latency statistics.</p></li>
<li><p>Logging of throughput and latency time series during the benchmark run in
standardized data formats.</p></li>
<li><p>Standardized plots to visualize the results post-benchmarking.</p></li>
</ul>
</section>
<section id="mybench-design">
<h3>mybench design<a class="headerlink" href="#mybench-design" title="Permalink to this heading">¶</a></h3>
<section id="internal-architecture">
<h4>Internal architecture<a class="headerlink" href="#internal-architecture" title="Permalink to this heading">¶</a></h4>
<p>mybench is a Golang library that enables developers to create, run, and analyze
benchmarks. To minimize the efforts required to define a benchmark, the
top-level API is kept small and much of the complexities of running a benchmark
is implemented within mybench. Figure 1 depicts the internal architecture of
mybench.</p>
<figure class="align-default" id="id9">
<img alt="_images/architecture.svg" src="_images/architecture.svg" /><figcaption>
<p><span class="caption-text">Figure 1: mybench architecture showing 3 <code class="docutils literal notranslate"><span class="pre">Workload</span></code>s each running with two
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>mybench defines a <strong>Benchmark</strong> to consists of multiple <strong>Workloads</strong>. At least
one Workload must be defined for a Benchmark. Each Workload is defined by its
<code class="docutils literal notranslate"><span class="pre">Event</span></code> callback, within which contains the code being benchmarked. For
example, the <code class="docutils literal notranslate"><span class="pre">Event</span></code> callback for one workload could implement a sequence of
queries that is typical for a web request to a particular API endpoint while
the <code class="docutils literal notranslate"><span class="pre">Event</span></code> callback for another workload could implement a different
sequence of queries that is typical for a different API endpoint. In the
figure, three such workloads are mixed together during the benchmark. Each
workload is configured by the number of parallel <code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s executing
the events and an overall event rate. In the figure, all three workloads are
configured to be executed with two parallel <code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s. The splitting
of workload into multiple parallel workers is useful for simulating scenarios
where many connections are sending queries to the database simultaneously.
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s are implemented as goroutines each with its own connection
to the database. The overall event rate is equally divided between all
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s. Each <code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code> embeds a <code class="docutils literal notranslate"><span class="pre">Looper</span></code> object,
which precisely controls the event rate (see <a class="reference internal" href="#rate-control-via-temporal-discretization-looper">Rate control via
“temporal-discretization” looper</a>), and an <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> object, which
stores the throughput and latency statistics during the benchmark run. Data
held inside the <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> objects are periodically read by the
<code class="docutils literal notranslate"><span class="pre">DataLogger</span></code> (see <a class="reference internal" href="#high-performance-data-logging-via-double-buffering">High-performance data logging via double buffering</a>).
Once the data is read, the <code class="docutils literal notranslate"><span class="pre">DataLogger</span></code> aggregates the throughput and latency
statistics across all <code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s and all <code class="docutils literal notranslate"><span class="pre">Workload</span></code>s into
per-workload and overall throughput and latency statistics. These values are
then written into a SQLite database.</p>
<p>Users developing a custom benchmark do not need to be concerned with the
internal details of mybench. To define a <code class="docutils literal notranslate"><span class="pre">Workload</span></code>, the user simply needs to
define a struct implementing the <code class="docutils literal notranslate"><span class="pre">WorkloadInterface</span></code> interface, which
requires the definition of the <code class="docutils literal notranslate"><span class="pre">Event</span></code> callback, a <code class="docutils literal notranslate"><span class="pre">Config</span></code> method
returning the configurations of the workload (such as the event rate and number
of parallel workers), and a <code class="docutils literal notranslate"><span class="pre">NewContextData</span></code> method that constructs a
custom, thread-local data structure that could be useful for the workload to
store thread-local state. Each custom benchmark is tied together with a
<code class="docutils literal notranslate"><span class="pre">BenchmarkApp</span></code> which constructs and owns the <code class="docutils literal notranslate"><span class="pre">Benchmark</span></code> object. Workloads
defined by the user can be added to the <code class="docutils literal notranslate"><span class="pre">Benchmark</span></code> through the
<code class="docutils literal notranslate"><span class="pre">BenchmarkApp</span></code> object.</p>
<p>Finally, the <code class="docutils literal notranslate"><span class="pre">BenchmarkApp</span></code> also handles other administrative duties common
between all custom-defined user benchmarks, such as parsing the command line
flags and setting an HTTP server where live throughput and latency statistics
can be monitored.</p>
<p>While mybench is primarily designed to benchmark MySQL, its architecture is
flexible enough to benchmark arbitrary database and non-database systems.
However, these are not uses cases that have been thoroughly tested for now.</p>
</section>
<section id="rate-control-via-temporal-discretization-looper">
<h4>Rate control via “temporal-discretization” looper<a class="headerlink" href="#rate-control-via-temporal-discretization-looper" title="Permalink to this heading">¶</a></h4>
<p>A common approach to discover the limits of the database is by performing a
sequence of benchmarks with increasing event rates. When the database is
overloaded, the event rate will plateau and the latency will stagnate. To
perform this type of benchmarks, an accurate and precise rate controller is
required. One naive approach to implement such a controller would be executing
the event code followed by a sleep in a simple loop. The sleep duration can be
calculated to match the requested event rate. However, maintaining a loop like
with a loop rate beyond 500 - 1000 Hz is difficult due to scheduling latencies
incurred by the Linux process scheduler without the real-time patch applied.
Additionally, Go’s goroutine scheduler can introduce additional latencies on the
order of about 3 ms (based on experimental testing while developing mybench),
which further increases the difficulties of maintaining a high event rate.</p>
<p>Thus, instead of calling <code class="docutils literal notranslate"><span class="pre">Event</span></code> once per loop iteration, mybench implements
two nested loops where the inner loop calls <code class="docutils literal notranslate"><span class="pre">Event</span></code> multiple times while the
outer loop maintains a constant and relatively low outer loop frequency of 50 Hz
(by default). The number of <code class="docutils literal notranslate"><span class="pre">Event</span></code> calls is calculated by simulating the
arrival of events within the outer loop iteration based on the desired event
rate by sampling from either a uniform or a Poisson distribution. Figure 2a
depicts this sampling process with a uniform event arrival distribution. After
calling <code class="docutils literal notranslate"><span class="pre">Event</span></code> in the inner loop, the looper sleeps until the next scheduled
wake-up time according to the outer loop frequency, when it then repeats the
same process for the next iteration.</p>
<figure class="align-default" id="id10">
<img alt="_images/looper.svg" src="_images/looper.svg" /><figcaption>
<p><span class="caption-text">Figure 2: The temporal-discretization scheme mybench rate controller a) in
normal operating circumstances and b) when the events are too slow to keep at
a desired event rate. Each box is an <code class="docutils literal notranslate"><span class="pre">Event</span></code> call.</span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In cases where <code class="docutils literal notranslate"><span class="pre">Event</span></code> call durations are too long to maintain a particular
desired event rate, the looper will detect this overrun and switch into an
overload mode (Figure 2b) where each outer loop iteration only calls <code class="docutils literal notranslate"><span class="pre">Event</span></code>
once. The sleep between successive outer loop iterations is also removed to
maximize the event rate. The looper detects this overrun effectively by tracking
the number of events it should have executed based on the desired event rate
since the start of the run and compare it against the number of events it has
actually executed <a class="footnote-reference brackets" href="#flooperoverload" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. If the number of events executed is lower
than the number of events it should have executed, the looper switches into the
overload mode. If at any point the actual number of events executed catches
up with the expected number of events, the looper will switch back into its
normal mode where events are batched. This system allows the looper to maintain
the average event rate over a long period of time in cases where the <code class="docutils literal notranslate"><span class="pre">Event</span></code>
call duration momentarily increases beyond the threshold at which the desired
event rate could no longer be sustained.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="flooperoverload" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">2</a><span class="fn-bracket">]</span></span>
<p>This is actually implemented slightly differently in the
code. The code currently implements something effectively similar but may need
to be updated to this description to be more correct. This is a TODO.</p>
</aside>
</aside>
</section>
<section id="high-performance-data-logging-via-double-buffering">
<h4>High-performance data logging via double buffering<a class="headerlink" href="#high-performance-data-logging-via-double-buffering" title="Permalink to this heading">¶</a></h4>
<p>The throughput and latency values of the <code class="docutils literal notranslate"><span class="pre">Event</span></code> calls are recorded into an HDR
histogram <a class="reference internal" href="#hdrhist01" id="id6"><span>[HDRHIST01]</span></a>, which is a data structure that can accurately track
histogram distributions with low memory and CPU overheads. Each
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code> records the latency value of each <code class="docutils literal notranslate"><span class="pre">Event</span></code> call in its own
instance of the HDR histogram, which is embedded the <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> struct
that also provides the capability to track the event throughput. Data in all
<code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> instances are collected and aggregated every one second such
that throughput and latency statistics over the course of the benchmark are
monitored as time series. Since each <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> instance is
continuously accessed and modified by their respective <code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>
goroutines, data collection must not introduce additional latencies or data races
to the main event loop.  Further, as mybench can simultaneously run hundreds of
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code> goroutines each with its own instance of
<code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code>, the time required to access all <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code>
instances must be short enough to not bias the calculation for the overall
throughput of the benchmark <a class="footnote-reference brackets" href="#fsnapshot" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<p>To satisfy these requirements, <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> is implemented as a double
buffer of the underlying HDR histogram objects. Figure 3 shows this double
buffer in operation. While the benchmark loop runs, the worker writes to the
active slot 1 of the double buffer (Figure 3a). Periodically, the data logger
takes a snapshot of the data for all <code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s. This is implemented
by swapping the active and inactive slot (Figure 3b). To avoid data races, a
per-worker mutex guards both writing to the active slot and the double buffer
swap operation. The swap operation is very fast, as it simply updates index
pointing to the current active slot. Immediately after the swap, the
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code> is unblocked and all data is now written to the newly active
slot 2 (Figure 3c). This occurs while the data logger finally reads the data
from the HDR histogram residing in the now-inactive slot 1. After data from all
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s are read, the data logger resets the histograms in the
inactive slots to a zero state such that they can be reused following the next
swap.</p>
<figure class="align-default" id="id11">
<img alt="_images/doublebuf.svg" src="_images/doublebuf.svg" /><figcaption>
<p><span class="caption-text">Figure 3: <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> as it is a) being written to while the data
logger is idling; b) performing the swap when the data logger takes a
data snapshot for all <code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>s; and c) being written to after
the swap and while the data logger reads from the inactive slot.</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Once the data is collected, the per-workload and aggregate throughput and
latency data are written into a SQLite database on disk. Each benchmark run
occupies a single table in the SQLite database. Meta information about the
benchmark run, such as the benchmark start and stop time, is written to a table
named <code class="docutils literal notranslate"><span class="pre">meta</span></code>. The choice of a SQLite database allows data from a series of
benchmark runs to be stored in a single file, which can simplify the transport,
storage, and post-processing of the data.</p>
<div role="list" class="citation-list">
<div class="citation" id="hdrhist01" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">HDRHIST01</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://hdrhistogram.org/">http://hdrhistogram.org/</a></p>
</div>
</div>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="fsnapshot" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">3</a><span class="fn-bracket">]</span></span>
<p>Throughput is calculated as number of events divided by the
elapsed time. Number of events is calculated by summing the event count in
all <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> instances. The elapsed time is calculated as the time
difference between two subsequent sampling of all <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code>
instances. If the time required to read all <code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> instances is
a large fraction of the elapsed time between two subsequent
<code class="docutils literal notranslate"><span class="pre">OnlineHistogram</span></code> sampling, this can introduce significant bias in the
calculated throughput, as both numerator and the denominator will be affected
by the passage of time.</p>
</aside>
</aside>
</section>
<section id="random-data-generation">
<h4>Random data generation<a class="headerlink" href="#random-data-generation" title="Permalink to this heading">¶</a></h4>
<p>Database benchmarks require the generation of random data. Similar to other
benchmark systems, mybench also provides a number of builtin data generators
that can generate data with different types and distributions. All data
generators implement an interface two methods: (1) one for the generation of
“new” data to be written to the database and (2) one for sampling data
that already exists on the database such that the data can be used in the
<code class="docutils literal notranslate"><span class="pre">WHERE</span></code> clause of a <code class="docutils literal notranslate"><span class="pre">SELECT</span></code> statement. At this time, the data generator
implemented within mybench are relatively primitive. For example, accurately
sampling existing data can be memory and time intensive and mybench does not
implement a fully-correct version of the existing data sampling algorithm at
this time. Instead, data sampling is done in a best-effort manner to ensure
the high performance and small resource footprint of mybench is not
compromised.</p>
<p>The random data generators implement in mybench uses the standard <code class="docutils literal notranslate"><span class="pre">rand</span></code>
library from Go’s standard libraries. The default random number generator
implemented in the <code class="docutils literal notranslate"><span class="pre">rand</span></code> library uses a global random source protected by a
global mutex. Since random data generation are performed concurrently from
every worker, the global mutex protecting a global random source creates a
significant performance bottleneck. This is resolved in mybench as data
generators uses an non-protected, gouroutine-local random source stored on each
<code class="docutils literal notranslate"><span class="pre">BenchmarkWorker</span></code>.</p>
</section>
<section id="live-monitoring-user-interface">
<h4>Live monitoring user interface<a class="headerlink" href="#live-monitoring-user-interface" title="Permalink to this heading">¶</a></h4>
<p>mybench implements a web-based user interface that displays time series for
throughput and latency of the running benchmark in real-time. Every five
seconds, the user interface requests the throughput and latency time series for
the current benchmark via HTTP. These time series are gathered by the data
logger periodically and stored within a ring buffer. Visualization of the time
series is implemented with the VegaLite visualization framework <a class="reference internal" href="#vega01" id="id8"><span>[VEGA01]</span></a>. This
user interface allows users to identify issues with their custom benchmarks
more quickly and therefore shortens the overall time required to develop and
conduct benchmarks. Figure 4 shows a screenshot of this user interface.</p>
<figure class="align-default" id="id12">
<img alt="_images/screenshot.png" src="_images/screenshot.png" />
<figcaption>
<p><span class="caption-text">Figure 4: A snapshot of the live monitoring user interface</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div role="list" class="citation-list">
<div class="citation" id="vega01" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">VEGA01</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://vega.github.io/vega-lite/">https://vega.github.io/vega-lite/</a></p>
</div>
</div>
</section>
<section id="post-processing-tools">
<h4>Post-processing tools<a class="headerlink" href="#post-processing-tools" title="Permalink to this heading">¶</a></h4>
<p>mybench includes a suite of post-processing utilities that can generate
plots to visualize a benchmark run’s throughput and latency statistics.
Multiple benchmark runs can also be visualized on the same figure to better
compare them. Figure 5 shows an instance of this comparison with a sequence of
benchmark runs where the desired throughput is increased from run to run.</p>
<figure class="align-default" id="id13">
<img alt="_images/postprocessing.png" src="_images/postprocessing.png" />
<figcaption>
<p><span class="caption-text">Figure 5: A figure generated by the post-processing tools that show the
throughput and latency of a sequence of benchmarks with increasing desired
throughput (<em>d</em>).</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="experimental-evaluations-and-discussions">
<h3>Experimental evaluations and discussions<a class="headerlink" href="#experimental-evaluations-and-discussions" title="Permalink to this heading">¶</a></h3>
<section id="rate-control-stability">
<h4>Rate control stability<a class="headerlink" href="#rate-control-stability" title="Permalink to this heading">¶</a></h4>
</section>
<section id="mybench-resource-utilization">
<h4>mybench resource utilization<a class="headerlink" href="#mybench-resource-utilization" title="Permalink to this heading">¶</a></h4>
</section>
<section id="ease-of-implementation-of-new-benchmarks">
<h4>Ease of implementation of new benchmarks<a class="headerlink" href="#ease-of-implementation-of-new-benchmarks" title="Permalink to this heading">¶</a></h4>
</section>
</section>
<section id="limitations-and-future-work">
<h3>Limitations and future work<a class="headerlink" href="#limitations-and-future-work" title="Permalink to this heading">¶</a></h3>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h3>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">mybench</a></h1>



<p class="blurb">A high-performance framework for rapid prototyping of benchmarks</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=Shopify&repo=mybench&type=star&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">What is mybench?</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="writing-a-benchmark.html">Tutorial: Writing a custom benchmark</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Detailed design documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#mybench-a-high-performance-framework-for-rapid-prototyping-of-benchmarks">mybench: a high-performance framework for rapid prototyping of benchmarks</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="writing-a-benchmark.html" title="previous chapter">Tutorial: Writing a custom benchmark</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022 - Present, Shopify Inc..
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/detailed-design-doc.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>